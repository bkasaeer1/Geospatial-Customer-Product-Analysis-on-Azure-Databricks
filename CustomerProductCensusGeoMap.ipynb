{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High-Performance Customer Product Big Data Analysis on Azure Databricks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose you are a business owner with several product categories used by numerous customers across various locations within the USA. Assuming you have a table containing the geographic coordinates of your customers and another table summarizing all the services available for those customers, the following script provides insights into the product quality experienced by your customers at different Census geographic levels (i.e., State, County, Tract, Block Group, and Block). This template is designed in a modular form to be executed as a scheduled notebook in Azure Databricks, utilizing the high-performance Delta table format. In this scenario, it is assumed that the products are categorized as A, B, or C, with A being the highest quality and C being the lowest. The customer_id is a unique identifier representing individual customers across the USA, identified by their latitude and longitude, facilitating the association between the product and customer tables before aggregation and analysis. The product power is considered an indicator of its quality for each product (A, B, or C), and we set the following standards for products consumed by users: Bronze products have a maximum power greater than or equal to 100 and a minimum power greater than or equal to 50, Gold products have a maximum power greater than or equal to 1000 and a minimum power greater than or equal to 200, while Silver products fall between Bronze and Gold. The script has been tested on terabyte-sized data tables running on a 16-node cluster with 128 GB RAM on Azure Databricks, and the entire aggregation process for all Census geographic levels, including the time required to write the CSV output results to Azure Blob storage, was completed in approximately 45 minutes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a5c37bad-71e4-45c8-8f3e-3e7a03340fd2",
     "showTitle": true,
     "title": "Imports & Configurations"
    }
   },
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "from glob import glob\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as f\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "pd.set_option('display.precision', 2) \n",
    "pd.set_option('display.max_columns', None)\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#product model with their corresponding code, in terms of quality A > B > C! \n",
    "product_model_dict = {4:'C', 2:'B', 1: 'A'}\n",
    "#also define the geographic leves and their number of digits! \n",
    "subgeo_dict = {'Blocks':15, 'Block Groups':12, 'Tracts':11, 'Counties':5, 'States':2}\n",
    "#specify the version of products dataset!\n",
    "product_data_version = '211'\n",
    "#specify the version of customers data!\n",
    "customer_data_version = 'v2'\n",
    "#All means the entire USA will be processed. To do only one state write state fips code like 11 for DC.\n",
    "state_fips = 'All'\n",
    "\n",
    "#if you did not have delta in hand and want to generate it for future faster runs below should be True!\n",
    "#otherwise if you prefer to run it from csvs, make below False and make sure all CSV sources in section below are valid!\n",
    "generateDelta = True\n",
    "\n",
    "############Specify input and output directories within mounted drives\n",
    "#path to all delta tables!\n",
    "deltaPath = '/mnt/Delta'\n",
    "#Specify the mounted folder in which you like your outputs to be stored in! \n",
    "outputCSV_location = '/mnt/output/%s'%product_data_version\n",
    "baseDataCSV_location = \"/mnt/base/baselines\"\n",
    "#specify the mounted folder in which the potnetial customers data are located\n",
    "customerDataCSV_location = \"/mnt/customer/%s\"%customer_data_version\n",
    "#specify the mounted folder in which your products data is located\n",
    "productDataCSV_location = \"/mnt/product/v%s\"%product_data_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2645a624-0ba6-4d7b-9bce-38c988c4584d",
     "showTitle": true,
     "title": "States Table Creator Function "
    }
   },
   "outputs": [],
   "source": [
    "def create_state_sdf():\n",
    "    \"\"\"\n",
    "    This function creates a spark dataframe containing state names, abbreviation, and fips code!\n",
    "\n",
    "    INPUTS:\n",
    "    This function has no inputs as the inputs are defined inside it. \n",
    "\n",
    "    OUTPUTS: \n",
    "    a spark dataframe containing the state info!\n",
    "    \"\"\"\n",
    "    # This includes US state names and fips code. Source: https://www.census.gov/library/reference/code-lists/ansi.html\n",
    "    state_dict = {'01': 'Alabama', '02': 'Alaska', '04': 'Arizona', '05': 'Arkansas', '06': 'California', '08': 'Colorado', '09': 'Connecticut', '10': 'Delaware', \n",
    "                '11': 'District of Columbia', '12': 'Florida', '13': 'Georgia', '15': 'Hawaii', '16': 'Idaho', '17': 'Illinois', '18': 'Indiana', '19': 'Iowa', \n",
    "                '20': 'Kansas', '21': 'Kentucky', '22': 'Louisiana', '23': 'Maine', '24': 'Maryland', '25': 'Massachusetts', '26': 'Michigan', '27': 'Minnesota', \n",
    "                '28': 'Mississippi', '29': 'Missouri', '30': 'Montana', '31': 'Nebraska', '32': 'Nevada', '33': 'New Hampshire', '34': 'New Jersey', '35': 'New Mexico', \n",
    "                '36': 'New York', '37': 'North Carolina', '38': 'North Dakota', '39': 'Ohio', '40': 'Oklahoma', '41': 'Oregon', '42': 'Pennsylvania', '44': 'Rhode Island',\n",
    "                '45': 'South Carolina', '46': 'South Dakota', '47': 'Tennessee', '48': 'Texas', '49': 'Utah', '50': 'Vermont', '51': 'Virginia', '53': 'Washington', \n",
    "                '54': 'West Virginia', '55': 'Wisconsin', '56': 'Wyoming', '60': 'American Samoa', '66': 'Guam', '69': 'Commonwealth of the Northern Mariana Islands', \n",
    "                '72': 'Puerto Rico', '78': 'United States Virgin Islands'}\n",
    "\n",
    "    state_abbr_dict = {'01': 'AL', '02': 'AK', '04': 'AZ', '05': 'AR', '06': 'CA', '08': 'CO', '09': 'CT', '10': 'DE', \n",
    "                '11': 'DC', '12': 'FL', '13': 'GA', '15': 'HI', '16': 'ID', '17': 'IL', '18': 'IN', '19': 'IA', \n",
    "                '20': 'KS', '21': 'KY', '22': 'LA', '23': 'ME', '24': 'MD', '25': 'MA', '26': 'MI', '27': 'MN', \n",
    "                '28': 'MS', '29': 'MO', '30': 'MT', '31': 'NE', '32': 'NV', '33': 'NH', '34': 'NJ', '35': 'NM', \n",
    "                '36': 'NY', '37': 'NC', '38': 'ND', '39': 'OH', '40': 'OK', '41': 'OR', '42': 'PA', '44': 'RI',\n",
    "                '45': 'SC', '46': 'SD', '47': 'TN', '48': 'TX', '49': 'UT', '50': 'VT', '51': 'VA', '53': 'WA', \n",
    "                '54': 'WV', '55': 'WI', '56': 'WY', '60': 'AS', '66': 'GU', '69': 'MP', \n",
    "                '72': 'PR', '78': 'VI'}\n",
    "    state_sdf = spark.createDataFrame(\n",
    "        pd.DataFrame({'STATEFP20': state_dict.keys(),\n",
    "                    'state_name': state_dict.values(),\n",
    "                    'state_abbr': state_abbr_dict.values()}))\n",
    "    return state_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56c7dbd2-6a53-45cb-b305-94b2ccee1534",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def convert_sdf_to_delta(sdf, deltaTablePath):\n",
    "    \"\"\"\n",
    "    This function converts the spark data table to delta table format to improve the performance! \n",
    "\n",
    "    INPUTS:\n",
    "    sdf: The spark df to be converted to delta format!  \n",
    "    delta_path: path to the delta table to be written!\n",
    "\n",
    "    OUTPUTS: \n",
    "    a delta table equivalent of the spark df! \n",
    "    \"\"\"\n",
    "    # write table to delta \n",
    "    sdf.write.format('delta').save(deltaTablePath)\n",
    "    return spark.read.load(deltaTablePath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4db2df3f-1632-4a87-83cf-c44015c7610b",
     "showTitle": true,
     "title": "Base Data Read Function "
    }
   },
   "outputs": [],
   "source": [
    "def read_base_data(subgeo, generateDelta = False):\n",
    "    \"\"\"\n",
    "    This function reads the base information for a specific subgeography using data previously generated from \n",
    "    past version of business dataset and is static. Pre loading this base data significantly reduces the \n",
    "    computation time since most of these parameters are static and won't change in a long period of time.\n",
    "\n",
    "    INPUTS:\n",
    "    subgeo: the target subgeography from a list of keys in subgeo_dict! \n",
    "    generateDelta: if delta doesn't exists, this allows the user to generate it and take advantage of it!\n",
    "\n",
    "    OUTPUTS: \n",
    "    a spark dataframe containing the base data for the target subgeography!\n",
    "    \"\"\"\n",
    "    \n",
    "    delta_location = deltaPath + \"/Base/%sBaseData.delta\"%subgeo\n",
    "    csv_location = baseDataCSV_location + \"/%sBaseData.csv\"%subgeo\n",
    "    try:\n",
    "        base_sdf = spark.read.load(delta_location)\n",
    "    except:  \n",
    "        \n",
    "        #read columns and customize schema \n",
    "        cols = spark.read.format('csv') \\\n",
    "                    .options(header = True, inferSchema = False) \\\n",
    "                    .option(\"sep\", \",\") \\\n",
    "                    .load(csv_location).columns\n",
    "\n",
    "        #define data types!\n",
    "        df_schema = StructType([StructField(c, StringType()) for c in cols])\n",
    "\n",
    "        base_sdf = spark.read.format('csv') \\\n",
    "                        .schema(df_schema) \\\n",
    "                        .options(header = True, inferSchema = False) \\\n",
    "                        .option(\"sep\", \",\") \\\n",
    "                        .load(csv_location) \n",
    "        #now if generate request is in place: \n",
    "        if generateDelta:\n",
    "            base_sdf = convert_sdf_to_delta(base_sdf, delta_location)\n",
    "            \n",
    "    return base_sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96a5ad4f-6e09-4283-af31-f39ae2325369",
     "showTitle": true,
     "title": "Location Fabric Data Read Function "
    }
   },
   "outputs": [],
   "source": [
    "def read_customer_data(customer_data_version, state_fips = 'All', generateDelta = False):\n",
    "    \"\"\"\n",
    "    This function reads the customer data geographic locations for the specified version of the table! \n",
    "\n",
    "    INPUTS:\n",
    "    customer_data_version: The customers data version!  \n",
    "    state_fips: this is to ensure processing can be narrowed down to a specific state for the testing phase. \n",
    "    Default is All means the entire USA but can be any fips code like 11, 51, etc.\n",
    "    generateDelta: if delta doesnt exists, this allows the user to generate it and take advantage of it!\n",
    "    \n",
    "    OUTPUTS: \n",
    "    a spark dataframe containing the customers positions for the specified version \n",
    "    \"\"\"\n",
    "    ##AllProducts: all business product positions\n",
    "    delta_location = deltaPath + '/customerLocations%s.delta'%customer_data_version    \n",
    "    csv_location = customerDataCSV_location + \"/%s.csv\"%customer_data_version\n",
    "    try:\n",
    "        AllCustomers = spark.read.load(delta_location)\n",
    "    except: \n",
    "\n",
    "        #read columns and customize schema \n",
    "        cols = spark.read.format('csv') \\\n",
    "                    .options(header = True, inferSchema = False) \\\n",
    "                    .option(\"sep\", \",\") \\\n",
    "                    .load(csv_location).columns\n",
    "\n",
    "        #In data dictionary of this dataset the customer id is mentioned as integer.\n",
    "        dtype_dict = {'customer_id': StringType(), 'latitude': FloatType(), 'longitude': FloatType()} \n",
    "\n",
    "        df_schema = StructType([StructField(c, dtype_dict[c]) if c in dtype_dict.keys() else StructField(\n",
    "            c, StringType()) for c in cols]) \n",
    "\n",
    "        AllCustomers = spark.read.format('csv') \\\n",
    "                        .schema(df_schema) \\\n",
    "                        .options(header = True, inferSchema = False) \\\n",
    "                        .option(\"sep\", \",\") \\\n",
    "                        .load(csv_location) \\\n",
    "                        .filter(f.col('customer_flag') == 'True')  \\\n",
    "                        .withColumnRenamed('block_geoid', 'GEOID')\n",
    "        #now if generate request is in place: \n",
    "        if generateDelta:\n",
    "            AllProducts = convert_sdf_to_delta(AllCustomers, delta_location)\n",
    "    #if one state is selected, then need to narrow down data to one state! \n",
    "    if state_fips != 'All':\n",
    "        AllCustomers = AllCustomers.filter(f.substring(f.col('GEOID'), 1, 2) == str(state_fips))\n",
    "    \n",
    "    return AllCustomers.select('customer_id','GEOID') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d5ed6a52-c2d0-495d-8017-a41717335867",
     "showTitle": true,
     "title": "BDC Data Read Function "
    }
   },
   "outputs": [],
   "source": [
    "def read_products_data(product_data_version, state_fips = 'All', generateDelta = False):\n",
    "    \"\"\"\n",
    "    This function reads business data for a specific version of the dataset and state. \n",
    "\n",
    "    INPUTS:\n",
    "    product_data_version: The products data version! \n",
    "    state_fips: this is to ensure processing can be narrowed down to a specific state. \n",
    "    Default is All means the entire USA but can be any fips code like 11, 55, etc.\n",
    "    generateDelta: if delta doesnt exists, this allows the user to generate it and take advantage of it!\n",
    "\n",
    "    OUTPUTS: \n",
    "    a spark dataframe containing the product details for the desired version of dataset. \n",
    "    \"\"\"\n",
    "    ##### Load product detailed Data #####\n",
    "    delta_location = deltaPath + '/product%s.delta'%product_data_version\n",
    "    csv_location = productDataCSV_location + \"/product_*.csv\"\n",
    "\n",
    "    try:\n",
    "        productSDF = spark.read.load(delta_location)\n",
    "    except: \n",
    "        #read columns and customize schema \n",
    "        cols = spark.read.format('csv') \\\n",
    "                    .options(header = True, inferSchema = False) \\\n",
    "                    .option(\"sep\", \",\") \\\n",
    "                    .load(csv_location).columns\n",
    "        #product_quality: 1 means product has a warranty while 0 means it does not come with a warranty.\n",
    "        dtype_dict= {'business_id': StringType(),'customer_id': StringType(), 'block_geoid': StringType(), \n",
    "                          'product_model': IntegerType(), 'product_max_power': IntegerType(),'product_min_power': IntegerType(), \n",
    "                          'product_quality': IntegerType()}\n",
    "\n",
    "        df_schema = StructType([StructField(c, dtype_dict[c]) for  c in cols]) \n",
    "\n",
    "        productSDF = spark.read.format('csv') \\\n",
    "                    .schema(df_schema) \\\n",
    "                    .option(\"mode\", \"FAILFAST\") \\\n",
    "                    .options(header = True, inferSchema = False) \\\n",
    "                    .option(\"quote\", \"\\\"\") \\\n",
    "                    .option(\"escape\", \"\\\"\") \\\n",
    "                    .load(csv_location) \n",
    "                    \n",
    "\n",
    "        #now if generate request is in place: \n",
    "        if generateDelta:\n",
    "            productSDF = convert_sdf_to_delta(productSDF, delta_location)\n",
    "    #if one state is selected, then need to narrow down data to one state! \n",
    "\n",
    "    if state_fips != 'All':\n",
    "        productSDF = productSDF.filter(f.substring(f.col('block_geoid'), 1, 2) == str(state_fips))\n",
    "    return productSDF.select('customer_id', 'block_geoid', 'product_model', 'product_max_power',\n",
    "                            'product_min_power', 'product_quality')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5fb68a04-141f-4a1d-b46a-abf91f8a414b",
     "showTitle": true,
     "title": "Sub-Geography Processor"
    }
   },
   "outputs": [],
   "source": [
    "def process_subgeo(subgeo):\n",
    "    \"\"\"\n",
    "    This function processes the product quality and availibility for a selected Census subgeography level!  \n",
    "\n",
    "    INPUTS:\n",
    "    subgeo: the target subgeography from a list of keys in subgeo_dict! \n",
    "\n",
    "    OUTPUTS: \n",
    "    a spark dataframe at the subgeo level containing the aggregated fields from products  \n",
    "    data in addition to the baseline information such as socio economic factors like ACS poverty, income etc. \n",
    "    \"\"\"\n",
    "    start = time.time()  \n",
    "    AllCustomers_subgeo = AllCustomers.withColumn('GEOID', f.substring(f.col('GEOID'), 1, subgeo_dict[subgeo])) \\\n",
    "                        .distinct() \n",
    "    #calculate aggregate the customers data to get the total bsls in each block! \n",
    "    TotalServices_subgeo = AllCustomers_subgeo.groupBy('GEOID').agg(f.count('customer_id').alias('TotalCustomers'))\n",
    "    #create a list to store all the triples of sdf! TotalCustomers should be there already!\n",
    "    sdfs = [[TotalServices_subgeo]]\n",
    "    \n",
    "    #now go through all the tech codes from model_dict and generate sdfs! \n",
    "    for product_model in ['All'] + list(product_model_dict.keys()):\n",
    "        #build the A/B/C triple!\n",
    "        quality_sdfs = quality_based_serving(TotalServices_subgeo, product_model)\n",
    "        #add it to the list!\n",
    "        sdfs.append(quality_sdfs)\n",
    "\n",
    "    #flatten the list of lists! \n",
    "    sdfs = list(chain(*sdfs))\n",
    "    #read in the base data for the subgeo!\n",
    "    base_sdf = read_base_data(subgeo, generateDelta)\n",
    "    #go in a loop and join all the sdfs \n",
    "    for sdf in sdfs:\n",
    "        base_sdf = base_sdf.join(sdf, on = 'GEOID', how = 'left')\n",
    "    #now write the output CSV to Azure blob storage!\n",
    "    _ = generate_output(base_sdf, subgeo)\n",
    "    \n",
    "    end = time.time()  \n",
    "    deltaT = end - start\n",
    "    print('product data processing for %s was accomplished in %s minutes!'%(\n",
    "        subgeo, round(deltaT/60, 1)))\n",
    "    return deltaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e62a39fd-c807-43b7-9c62-f4db3ab4941a",
     "showTitle": true,
     "title": "Serving Slicer"
    }
   },
   "outputs": [],
   "source": [
    "def quality_slicer(sdf, level = 'Bronze', model_code = 'All'):\n",
    "    \"\"\"\n",
    "    This function prepares a quality filter (A/B/C) using any specific spark df!\n",
    "\n",
    "    INPUTS:\n",
    "    sdf: the spark df to slice!\n",
    "    level: e.g. Gold! It can be Bronze, Silver, or Gold from the lowest to highest quality products. Default is Bronze. \n",
    "    model_code: the product model code! See the product_model_dict dictionary! The default is All meaning all models!\n",
    "\n",
    "    OUTPUTS: \n",
    "    A logical selector that helps with slicing the products data. \n",
    "    \"\"\"\n",
    "    sdf = sdf if model_code == 'All' else sdf.filter(sdf['product_model']== model_code) \n",
    "    \n",
    "    ####define quality level Logics By Min and Max Power###\n",
    "    power_level_dict = {'Bronze':[100, 50], 'Gold':[1000, 200]}\n",
    "    #here is the condition based on selected level to filter!\n",
    "    return ((sdf['product_max_power'] >= power_level_dict[level][0]) & \n",
    "                    (sdf['product_min_power'] >= power_level_dict[level][1]) & \n",
    "                    (sdf['product_quality'] == 1) &\n",
    "                    (sdf['product_model'].isin(list(product_model_dict.keys())) if model_code == 'All' else sdf[\n",
    "                        'product_model'] == model_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "831bba69-267d-4320-bc3f-a418ba83597d",
     "showTitle": true,
     "title": "Technology-Based Processor"
    }
   },
   "outputs": [],
   "source": [
    "def model_based_serving(AllCustomers_subgeo, model = 'All'):\n",
    "    \"\"\"\n",
    "    This function prepares a triple of serving sdfs (Bronze/Silver/Gold) for each model (A/B/C). For instance, \n",
    "    GoldB, SilverB, and BronzeB are built when model = B. See product_model_dict for all models and their code! \n",
    "\n",
    "    INPUTS:\n",
    "    AllBSLs_subgeo: a spark df containing all customers at the selected subgeography level!\n",
    "    model: the product model code! See the tech_dict dictionary! Default is All models meaning all models!\n",
    "\n",
    "    OUTPUTS: \n",
    "    three sdfs for the desired model (e.g. GoldB, SilverB, and BronzeB)! \n",
    "    \"\"\"\n",
    "    #look up the model name for use in attribute name! \n",
    "    model_name = '' if model == 'All' else product_model_dict[model]\n",
    "    ## BronzeCustomers: customers with Bronze products! \n",
    "    BronzeCustomers = AllCustomers_subgeo.join(AllCustomers.filter(quality_slicer(AllCustomers, 'Bronze', model_code)),\n",
    "                                how = 'left_anti',\n",
    "                                on = 'customer_id') \\\n",
    "                                .groupBy('GEOID') \\\n",
    "                                .agg(f.count('customer_id').alias('BronzeCustomers%s'%model_name))\n",
    "    ## SilverCustomers: customers with silver products! \n",
    "    SilverCustomers = AllCustomers_subgeo.join(AllCustomers.filter(quality_slicer(AllCustomers, 'Bronze', model_code)),\n",
    "                                how = 'left_semi',\n",
    "                                on = 'customer_id') \\\n",
    "                                    .join(AllCustomers.filter(quality_slicer(AllCustomers, 'Gold', model_code)),\n",
    "                                        how = 'left_anti',\n",
    "                                        on = 'customer_id') \\\n",
    "                                    .groupBy('GEOID') \\\n",
    "                                    .agg(f.count('customer_id').alias('SilverCustomers%s'%tech_name))\n",
    "    ## GoldCustomers\n",
    "    GoldCustomers = AllCustomers_subgeo.join(AllCustomers.filter(quality_slicer(AllCustomers, 'Gold', tech_code)),\n",
    "                                how = 'left_semi',\n",
    "                                on = 'customer_id') \\\n",
    "                                .groupBy('GEOID') \\\n",
    "                                .agg(f.count('customer_id').alias('GoldCustomers%s'%tech_name))\n",
    "    return [BronzeCustomers, SilverCustomers, GoldCustomers]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e37ff9cb-7cae-48b6-90ed-fd46d0523ac5",
     "showTitle": true,
     "title": "Output Generator"
    }
   },
   "outputs": [],
   "source": [
    "def generate_output(sdf, subgeo, writeDelta =False, writeCSV = True, splitByState = False):\n",
    "    \"\"\"\n",
    "    This function prepares a serving filter (BronzeCustomers, SilverCustomers, GoldCustomers) using any specific spark df!\n",
    "\n",
    "    INPUTS:\n",
    "    sdf: spark df to write as CSV!\n",
    "    subgeo: the target subgeography from a list of keys in subgeo_dict! \n",
    "    writeCSV: if false it writes it as Delta tables!\n",
    "    OUTPUTS: \n",
    "    returns the location of the saved CSV file for the selected subgeography!\n",
    "    \"\"\"\n",
    "    file_location_national = outputCSV_location + '/CustomerProduct%s%s%s.csv'%(product_data_version, subgeo, state_fips)\n",
    "    file_location_bystate = file_location_national[:-4] + '_ByState.csv'\n",
    "    #get a list of integer count columns! \n",
    "    customer_cols = [c for c in sdf.columns if 'Customer' in c]\n",
    "    #fill null values with zero and get ready for saving! \n",
    "    sdf_final = sdf.fillna(0, subset = bsl_cols).coalesce(1)\n",
    "\n",
    "    if writeCSV:\n",
    "        #now save the national file!\n",
    "        sdf_final.write.mode(\"overwrite\") \\\n",
    "                                .option(\"header\", True) \\\n",
    "                                .option(\"delimiter\", \",\") \\\n",
    "                                .csv(file_location_national)\n",
    "    if writeDelta:\n",
    "        file_location_national = deltaPath + '/%s/CustomerProduct%s%s%s.delta'%(product_data_version, product_data_version,\n",
    "                                                                                subgeo, state_fips)\n",
    "        _ = convert_sdf_to_delta(sdf_final, file_location_national)\n",
    "\n",
    "     \n",
    "    if splitByState:\n",
    "        #and save a parititioned by state version of it!\n",
    "        sdf_final.write \\\n",
    "                    .option(\"header\", True) \\\n",
    "                    .option(\"delimiter\", \",\") \\\n",
    "                    .partitionBy(\"StateName\") \\\n",
    "                    .mode(\"overwrite\") \\\n",
    "                    .csv(file_location_bystate)\n",
    "    return file_location_national"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff299b2e-335a-404c-a974-364ba34e6df7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    print('----Process Started----')\n",
    "    start = time.time()\n",
    "    #define below parameter to keep track of total processing time! \n",
    "    totalT = 0  \n",
    "    AllCustomers = read_customer_data(customer_data_version, state_fips, generateDelta).cache()\n",
    "    print('customer locations data version %s data was read as a spark dataframe!'%customer_data_version)\n",
    "    productSDF = read_products_data(product_data_version, state_fips,  generateDelta).cache()\n",
    "    print('products data version %s data was read as a spark dataframe!'%product_data_version)\n",
    "\n",
    "    #loop through all subgeos and generate results!\n",
    "    for subgeo in subgeo_dict.keys():\n",
    "        \n",
    "        print('Data processing for %s started!'%subgeo)\n",
    "        totalT += process_subgeo(subgeo)\n",
    "\n",
    "    end = time.time()  \n",
    "    deltaT = end - start\n",
    "    print('----Process Ended----')\n",
    "    print('----The sub-geography processing for products %s was %s minutes!'%(\n",
    "        product_data_version, round(totalT/60, 1)))\n",
    "    print('----The entire products %s process was accomplished in %s minutes!'%(\n",
    "        product_data_version, round(deltaT/60, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "NBAMByGeoFromBDC-Delta",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
